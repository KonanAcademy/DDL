{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. distributed tensorflow 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Konan HelloWorld / 분산딥러닝 스터디 [1, 3]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례\n",
    "* 1 실습 환경\n",
    "* 2 Hello distributed TensorFlow!\n",
    "* 3 공식 튜토리얼 간략 소개\n",
    "    - Create a cluster\n",
    "    - Specifying distributed devices in your model\n",
    "    - Specifying distributed devices in your model\n",
    "    - Replicated training\n",
    "    - Putting it all together: example trainer program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "\n",
    "##### tensorflow 개요\n",
    "* [1] Large Scale Deep Learning with TensorFlow - http://www.slideshare.net/JenAman/large-scale-deep-learning-with-tensorflow\n",
    "\n",
    "##### distributed tensorflow 개요\n",
    "* [2] Intro to the Distributed Version of TensorFlow - http://www.slideshare.net/altoros/intro-to-the-distributed-version-of-tensorflow\n",
    "* [3] Distributed TensorFlow (공식 홈페이지 튜토리얼) -https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html#distributed-tensorflow\n",
    "\n",
    "##### 앞으로의 실습 계획 관련 (docker)\n",
    "* [4] tensorflow 공식 docker hub - https://hub.docker.com/u/tensorflow/\n",
    "* [5] Tensorflow in Docker - http://www.slideshare.net/EricAhn/tensorflow-in-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 실습 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run -it --name run_dist_tf -p 8888:8888 -p 6006:6006 -v [your_dir_path]:/notebooks/work/ gcr.io/tensorflow/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hello distributed TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start a TensorFlow server as a single-process \"cluster\".\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tf.constant(\"Hello, distributed TensorFlow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server = tf.train.Server.create_local_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(server.target)  # Create a session on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, distributed TensorFlow!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 공식 튜토리얼 간략 소개\n",
    "* Create a cluster\n",
    "* Specifying distributed devices in your model\n",
    "* Specifying distributed devices in your model\n",
    "* Replicated training\n",
    "* Putting it all together: example trainer program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.train.ClusterSpec\n",
    "* tf.train.Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.train.ClusterSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Create a tf.train.ClusterSpec</font> that describes all of the tasks in the cluster. This should be the same for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예제 1 \n",
    "tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예제 2\n",
    "tf.train.ClusterSpec({\n",
    "    \"worker\": [\n",
    "        \"worker0.example.com:2222\",\n",
    "        \"worker1.example.com:2222\",\n",
    "        \"worker2.example.com:2222\"\n",
    "    ],\n",
    "    \"ps\": [\n",
    "        \"ps0.example.com:2222\",\n",
    "        \"ps1.example.com:2222\"\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train.Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Create a tf.train.Server</font>, passing the tf.train.ClusterSpec to the constructor, and identifying the local task with a job name and task index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to launch a cluster with two servers running on localhost:2222 and localhost:2223, run the following snippets in two different processes on the local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In task 0:\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In task 1:\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying distributed devices in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/job:ps/task:0\"):\n",
    "  weights_1 = tf.Variable(...)\n",
    "  biases_1 = tf.Variable(...)\n",
    "\n",
    "with tf.device(\"/job:ps/task:1\"):\n",
    "  weights_2 = tf.Variable(...)\n",
    "  biases_2 = tf.Variable(...)\n",
    "\n",
    "with tf.device(\"/job:worker/task:7\"):\n",
    "  input, labels = ...\n",
    "  layer_1 = tf.nn.relu(tf.matmul(input, weights_1) + biases_1)\n",
    "  logits = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)\n",
    "  # ...\n",
    "  train_op = ...\n",
    "\n",
    "with tf.Session(\"grpc://worker7.example.com:2222\") as sess:\n",
    "  for _ in range(10000):\n",
    "    sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicated training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In-graph replication\n",
    "* Between-graph replication\n",
    "* Asynchronous training\n",
    "* Synchronous training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: example trainer program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Flags for defining the tf.train.ClusterSpec\n",
    "tf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "tf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flags for defining the tf.train.Server\n",
    "tf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\n",
    "tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Flags for defining the tf.train.ClusterSpec\n",
    "tf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "tf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "\n",
    "# Flags for defining the tf.train.Server\n",
    "tf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\n",
    "tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  ps_hosts = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_hosts = FLAGS.worker_hosts.split(\",\")\n",
    "\n",
    "  # Create a cluster from the parameter server and worker hosts.\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n",
    "\n",
    "  # Create and start a server for the local task.\n",
    "  server = tf.train.Server(cluster,\n",
    "                           job_name=FLAGS.job_name,\n",
    "                           task_index=FLAGS.task_index)\n",
    "\n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "\n",
    "    # Assigns ops to the local worker by default.\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "        cluster=cluster)):\n",
    "\n",
    "      # Build model...\n",
    "      loss = ...\n",
    "      global_step = tf.Variable(0)\n",
    "\n",
    "      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n",
    "          loss, global_step=global_step)\n",
    "\n",
    "      saver = tf.train.Saver()\n",
    "      summary_op = tf.merge_all_summaries()\n",
    "      init_op = tf.initialize_all_variables()\n",
    "\n",
    "    # Create a \"supervisor\", which oversees the training process.\n",
    "    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n",
    "                             logdir=\"/tmp/train_logs\",\n",
    "                             init_op=init_op,\n",
    "                             summary_op=summary_op,\n",
    "                             saver=saver,\n",
    "                             global_step=global_step,\n",
    "                             save_model_secs=600)\n",
    "\n",
    "    # The supervisor takes care of session initialization, restoring from\n",
    "    # a checkpoint, and closing when done or an error occurs.\n",
    "    with sv.managed_session(server.target) as sess:\n",
    "      # Loop until the supervisor shuts down or 1000000 steps have completed.\n",
    "      step = 0\n",
    "      while not sv.should_stop() and step < 1000000:\n",
    "        # Run a training step asynchronously.\n",
    "        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "        # perform *synchronous* training.\n",
    "        _, step = sess.run([train_op, global_step])\n",
    "\n",
    "    # Ask for all the services to stop.\n",
    "    sv.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the trainer with two parameter servers and two workers, use the following command line (assuming the script is called trainer.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On ps0.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=ps --task_index=0\n",
    "# On ps1.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=ps --task_index=1\n",
    "# On worker0.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=worker --task_index=0\n",
    "# On worker1.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=worker --task_index=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 \n",
    "* [1] Large Scale Deep Learning with TensorFlow - http://www.slideshare.net/JenAman/large-scale-deep-learning-with-tensorflow\n",
    "* [2] Intro to the Distributed Version of TensorFlow - http://www.slideshare.net/altoros/intro-to-the-distributed-version-of-tensorflow\n",
    "* [3] Distributed TensorFlow (공식 홈페이지 튜토리얼) -https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html#distributed-tensorflow\n",
    "* [4] tensorflow 공식 docker hub - https://hub.docker.com/u/tensorflow/\n",
    "* [5] Tensorflow in Docker - http://www.slideshare.net/EricAhn/tensorflow-in-docker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
